# ap_words 応用情報技術者試験シラバス用・用語例スクリプト

応用情報技術者試験のシラバス(用語例の塊) から「用語例」を抽出し、AIにそれぞれの用語についての解説を作ってもらうためのテキストファイルを作ります。
高度情報のシラバスは見た事がないので不明ですが、基本情報とかのシラバスを読み込ませても同様に動作すると思われます。
注意: AIに用語例解説をお願いするテキストファイルを作るだけのものであって、AIとのやり取りをするスクリプトではない。

-----

## 機能 📝

  * **見出しの自動認識**: `(2) 見出し1` や `① 見出し2` といった見出し部を認識し、AIに質問する際のキーワードとします。
  * **用語例の抽出**: 「**用語例**」というキーワードの後に続く用語を「、」区切りで抽出します。
  * **括弧内のカンマは無視**: `（...、...）` のように括弧で囲まれた中のカンマは、用語区切りの対象とはしません。
  * **プロンプト形式への変換**: 抽出した用語を、AIアシスタントに解説を依頼するためのプロンプトテキストとして出力します。

-----

## 使い方 🚀

Python 3が必要です。
また、IPAのシラバスを全選択してコピペしたテキストファイルも必要です。

### 1\. ファイルの準備

以下の形式で用語を記載したテキストファイル（例: `ap_syl.txt`）を用意します。
具体的にはIPAのPDF化されているシラバスを全選択してテキストファイルにコピペします。

```text
(1) 情報セキュリティ
① 情報資産の調査
用語例
資産、脅威、脆弱性

(2) リスク分析と評価
用語例
リスクアセスメント、リスク対応
```

### 2\. スクリプトの実行

ターミナルまたはコマンドプロンプトで、以下のようなコマンドを実行します。
ここでは　ap_syl.txt が前節記載のシラバスからのコピペテキストであるとします。

```sh
python ap_words.py ap_syl.txt > apwords.txt
```

### 3\. 出力結果

実行すると、以下のような形式で出力されます。テキストエディタなどから適宜コピーして、ChatGPTやGemini等にお願いします。

```text
_ask
お願い文章
用語1
用語2
用語n
```

```text
_ask
応用情報処理試験の出題範囲「AI（Artificial Intelligence：人工知能）に関する技術」における「ディープラーニング（深層学習）」について、
以下の用語に関する解説(基本的には400文字以内で、内容が複雑な場合は最大600文字、
意味が単純なら文字数に合わせず簡潔にまとめてもいい)を、表形式でまとめてください。
なお、文体は簡潔にするために、「だ・である」系や体言止めでお願いします。
ニューラルネットワーク
多層パーセプトロン
バックプロパゲーション（誤差逆伝播法）
勾配消失問題
活性化関数（ReLU 関数
ソフトマックス関数
tanh関数など）
ドロップアウト
事前学習
ファインチューニング
転移学習
バッチ学習
ミニバッチ学習
オンライン学習
```

参考例: Geminiの回答
応用情報技術者試験の出題範囲「AI（人工知能）に関する技術」について、ディープラーニングに関する用語の解説を表形式で以下にまとめる。

| 用語 | 解説 |
|:---|:---|
| **ニューラルネットワーク** | 人間の脳の神経細胞（ニューロン）の仕組みを模倣した数学的モデル。入力層、隠れ層、出力層から構成され、層内の各ニューロンが重みとバイアスを調整して学習する。多層パーセプトロンや畳み込みニューラルネットワークなどの基盤となる。 |
| **多層パーセプトロン** | ニューラルネットワークの最も基本的な構造。複数の隠れ層を持つことで、入力と出力の間の非線形な関係を学習できる。各層のニューロンが前の層のすべてのニューロンと結合しているため、全結合型ニューラルネットワークとも呼ばれる。 |
| **バックプロパゲーション（誤差逆伝播法）** | ニューラルネットワークの学習アルゴリズムの一つ。出力層で計算された予測値と正解データの誤差を算出し、その誤差を逆方向に（出力層から入力層へ）伝播させる。この誤差情報をもとに、各ニューロンの重みとバイアスを効率的に更新する。 |
| **勾配消失問題** | バックプロパゲーションにおいて、誤差が入力層に近い層に伝わるにつれて、勾配（重み更新の度合い）が非常に小さくなり、学習が進まなくなる現象。特に、層の数が多いディープラーニングで顕著に現れる。活性化関数としてReLU関数を使用することで、この問題は大幅に緩和される。 |
| **活性化関数** | ニューロンの出力値を決定する関数。線形な出力を非線形に変換することで、複雑な関係性の学習を可能にする。ReLU関数（入力が正ならそのまま、負なら0を出力）、ソフトマックス関数（複数のクラスの確率を出力）、tanh関数（出力を-1から1の間に正規化）などがある。 |
| **ドロップアウト** | 過学習を抑制するための正則化手法。学習時に、各層のニューロンを一定の確率でランダムに無効化することで、特定のニューロンに依存しすぎることを防ぎ、ネットワークの汎化性能を高める。 |
| **事前学習** | 大規模なデータセットを用いて、教師なし学習などでネットワークの重みを事前に調整する学習方法。これにより、特徴抽出器として機能するネットワークの基礎が作られる。 |
| **ファインチューニング** | 事前学習済みのモデルを、特定のタスク（例：猫の画像認識）に適用するために、少量の教師ありデータで重みを微調整する学習方法。学習効率が良く、少量データでも高い精度を達成できる。 |
| **転移学習** | あるタスクで学習したモデルの知識（重みや構造）を、別の関連タスクに転用する手法。ファインチューニングは転移学習の一種。ゼロから学習するよりもはるかに効率的で、計算コストと時間の大幅な削減につながる。 |
| **バッチ学習** | 全ての訓練データセットを一度にまとめて学習させる方法。学習の安定性が高い反面、データ量が多いとメモリ消費が大きくなる。 |
| **ミニバッチ学習** | 全てのデータではなく、訓練データをいくつかのグループ（ミニバッチ）に分割して、グループごとに学習させる方法。バッチ学習とオンライン学習の中間的な手法であり、学習の安定性とメモリ効率のバランスが良い。 |
| **オンライン学習** | 訓練データを1つずつネットワークに入力して学習させる方法。メモリ効率が最も高い反面、学習が不安定になりやすい。ストリーミングデータなど、逐次的にデータが手に入る場合に有効。 |

-----


このプロジェクトは、**MIT License**の下で公開されています。詳細については、`LICENSE`ファイルを参照してください。
